{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gym\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython import display\n",
    "from collections import deque\n",
    "from skimage.color import rgb2grey\n",
    "from skimage.transform import rescale\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, num_frames, num_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=num_frames,\n",
    "            out_channels=16,\n",
    "            kernel_size=8,\n",
    "            stride=4,\n",
    "            padding=2\n",
    "            )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "            )\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=3200,\n",
    "            out_features=256,\n",
    "            )\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=256,\n",
    "            out_features=num_actions,\n",
    "            )\n",
    "        \n",
    "        # Activations\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def flatten(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Forward pass\n",
    "        x = self.relu(self.conv1(x)) # In: (80, 80, 4), Out: (20, 20, 16)\n",
    "        x = self.relu(self.conv2(x)) # In: (20, 20, 16), Out: (10, 10, 32)\n",
    "        x = self.flatten(x) # In: (10, 10, 32), Out: (3200,)\n",
    "        x = self.relu(self.fc1(x)) # In: (3200,), Out: (256,)\n",
    "        x = self.fc2(x) # In: (256,), Out: (4,)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_size(w, k, s, p):\n",
    "    return ((w - k + 2*p)/s) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, memory_depth, lr, gamma, epsilon_i, epsilon_f, anneal_time, ckptdir):\n",
    "        \n",
    "        self.cuda = True if torch.cuda.is_available() else False\n",
    "        self.to_tensor = torch.cuda.FloatTensor if self.cuda else torch.FloatTensor\n",
    "        self.to_byte_tensor = torch.cuda.ByteTensor if self.cuda else torch.ByteTensor\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "        self.memory_depth = memory_depth\n",
    "        self.gamma = self.to_tensor([gamma])\n",
    "        self.e_i = epsilon_i\n",
    "        self.e_f = epsilon_f\n",
    "        self.anneal_time = anneal_time\n",
    "        self.ckptdir = ckptdir\n",
    "        \n",
    "        if not os.path.isdir(ckptdir):\n",
    "            os.makedirs(ckptdir)\n",
    "        \n",
    "        self.memory = deque(maxlen=memory_depth)\n",
    "        self.clone()\n",
    "        \n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "        self.opt = torch.optim.RMSprop(self.model.parameters(), lr=lr, alpha=0.95, eps=0.01)\n",
    "        \n",
    "    def clone(self):\n",
    "        try:\n",
    "            del self.clone_model\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.clone_model = copy.deepcopy(self.model)\n",
    "        \n",
    "        for p in self.clone_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        if self.cuda:\n",
    "            self.clone_model = self.clone_model.cuda()\n",
    "    \n",
    "    def remember(self, state, action, reward, terminal, next_state):\n",
    "        \n",
    "        if self.cuda:\n",
    "            state, next_state = state.cpu(), next_state.cpu()\n",
    "            \n",
    "        state, next_state = state.data.numpy(), next_state.data.numpy()\n",
    "        state, next_state = 255.*state, 255.*next_state\n",
    "        state, next_state = state.astype(np.uint8), next_state.astype(np.uint8)\n",
    "        \n",
    "        self.memory.append([state, action, reward, terminal, next_state])\n",
    "    \n",
    "    def retrieve(self, batch_size):\n",
    "        \n",
    "        if batch_size > self.memories:\n",
    "            batch_size = self.memories\n",
    "        \n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        state = np.concatenate([batch[i][0] for i in range(batch_size)]).astype(np.int64)\n",
    "        action = np.array([batch[i][1] for i in range(batch_size)], dtype=np.int64)[:, None]\n",
    "        reward = np.array([batch[i][2] for i in range(batch_size)], dtype=np.int64)[:, None]\n",
    "        terminal = np.array([batch[i][3] for i in range(batch_size)], dtype=np.int64)[:, None]\n",
    "        next_state = np.concatenate([batch[i][4] for i in range(batch_size)]).astype(np.int64)\n",
    "        \n",
    "        state = self.to_tensor(state/255.)\n",
    "        next_state = self.to_tensor(next_state/255.)\n",
    "        reward = self.to_tensor(reward)\n",
    "        terminal = self.to_byte_tensor(terminal)\n",
    "\n",
    "        return state, action, reward, terminal, next_state\n",
    "    \n",
    "    @property\n",
    "    def memories(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def act(self, state):\n",
    "        q_values = self.model(state).detach()\n",
    "        action = torch.argmax(q_values)\n",
    "        return action.item()\n",
    "    \n",
    "    def process(self, state):\n",
    "        state = rgb2grey(state[35:195, :, :])\n",
    "        state = rescale(state, scale=0.5)\n",
    "        state = state[np.newaxis, np.newaxis, :, :]\n",
    "        return self.to_tensor(state)\n",
    "    \n",
    "    def exploration_rate(self, t):\n",
    "        if 0 <= t < self.anneal_time:\n",
    "            return self.e_i - t*(self.e_i - self.e_f)/self.anneal_time\n",
    "        elif t >= self.anneal_time:\n",
    "            return self.e_f\n",
    "        elif t < 0:\n",
    "            return self.e_i\n",
    "    \n",
    "    def save(self, t):\n",
    "        save_path = os.path.join(self.ckptdir, 'model-{}'.format(t))\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "    \n",
    "    def load(self):\n",
    "        ckpts = [file for file in os.listdir(self.ckptdir) if 'model' in file]\n",
    "        steps = [int(re.search('\\d+', file).group(0)) for file in ckpts]\n",
    "        \n",
    "        latest_ckpt = ckpts[np.argmax(steps)]\n",
    "        self.t = np.max(steps)\n",
    "        \n",
    "        print(\"Loading checkpoint: {}\".format(latest_ckpt))\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(os.path.join(self.ckptdir, latest_ckpt)))\n",
    "        \n",
    "    def update(self, batch_size, verbose=False):\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        start = time.time()\n",
    "        state, action, reward, terminal, next_state = self.retrieve(batch_size)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Sampled memory in {:0.2f} seconds.\".format(time.time() - start))\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        q = self.model(state)[range(batch_size), action.flatten()][:, None]\n",
    "        qmax = self.clone_model(next_state).max(dim=1)[0][:, None]\n",
    "        \n",
    "        nonterminal_target = reward + self.gamma*qmax\n",
    "        terminal_target = reward\n",
    "        \n",
    "        target = terminal.float()*terminal_target + (~terminal).float()*nonterminal_target\n",
    "    \n",
    "        loss = self.loss(q, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Updated parameters in {:0.2f} seconds.\".format(time.time() - start))\n",
    "\n",
    "    def play(self, episodes, train=False, load=False, plot=False, render=False, verbose=False):\n",
    "    \n",
    "        self.t = 0\n",
    "        metadata = dict(episode=[], reward=[])\n",
    "        \n",
    "        if load:\n",
    "            self.load()\n",
    "\n",
    "        try:\n",
    "            progress_bar = tqdm(range(episodes), unit='episode')\n",
    "            \n",
    "            i = 0\n",
    "            for episode in progress_bar:\n",
    "\n",
    "                state = env.reset()\n",
    "                state = self.process(state)\n",
    "                \n",
    "                done = False\n",
    "                total_reward = 0\n",
    "\n",
    "                while not done:\n",
    "\n",
    "                    if render:\n",
    "                        env.render()\n",
    "\n",
    "                    while state.size()[1] < num_frames:\n",
    "                        action = 1 # Fire\n",
    "\n",
    "                        new_frame, reward, done, info = env.step(action)\n",
    "                        new_frame = self.process(new_frame)\n",
    "\n",
    "                        state = torch.cat([state, new_frame], 1)\n",
    "                    \n",
    "                    if train and np.random.uniform() < self.exploration_rate(self.t-burn_in):\n",
    "                        action = np.random.choice(num_actions)\n",
    "\n",
    "                    else:\n",
    "                        action = self.act(state)\n",
    "\n",
    "                    new_frame, reward, done, info = env.step(action)\n",
    "                    new_frame = self.process(new_frame)\n",
    "\n",
    "                    new_state = torch.cat([state, new_frame], 1)\n",
    "                    new_state = new_state[:, 1:, :, :]\n",
    "                    \n",
    "                    if train:\n",
    "                        self.remember(state, action, reward, done, new_state)\n",
    "\n",
    "                    state = new_state\n",
    "                    total_reward += reward\n",
    "                    self.t += 1\n",
    "                    i += 1\n",
    "                    \n",
    "                    if not train:\n",
    "                        time.sleep(0.1)\n",
    "                    \n",
    "                    if train and i > batch_size:\n",
    "\n",
    "                        if self.t % update_interval == 0:\n",
    "                            self.update(batch_size, verbose=verbose)\n",
    "\n",
    "                        if self.t % clone_interval == 0:\n",
    "                            self.clone()\n",
    "\n",
    "                        if self.t % save_interval == 0:\n",
    "                            self.save(self.t)\n",
    "\n",
    "                    if self.t % 1000 == 0:\n",
    "                        progress_bar.set_description(\"t = {}\".format(self.t))\n",
    "\n",
    "                metadata['episode'].append(episode)\n",
    "                metadata['reward'].append(total_reward)\n",
    "\n",
    "                if episode % 100 == 0 and episode != 0:\n",
    "                    avg_return = np.mean(metadata['reward'][-100:])\n",
    "                    print(\"Average return (last 100 episodes): {}\".format(avg_return))\n",
    "\n",
    "                if plot:\n",
    "                    plt.scatter(metadata['episode'], metadata['reward'])\n",
    "                    plt.xlim(0, episodes)\n",
    "                    plt.xlabel(\"Episode\")\n",
    "                    plt.ylabel(\"Return\")\n",
    "                    display.clear_output(wait=True)\n",
    "                    display.display(plt.gcf())\n",
    "            \n",
    "            env.close()\n",
    "            return metadata\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            if train:\n",
    "                print(\"Saving model before quitting...\")\n",
    "                self.save(self.t)\n",
    "            \n",
    "            env.close()\n",
    "            return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 32\n",
    "update_interval = 4\n",
    "clone_interval = int(1e4)\n",
    "save_interval = int(1e5)\n",
    "frame_skip = None\n",
    "num_frames = 4\n",
    "num_actions = 4\n",
    "episodes = int(1e5)\n",
    "memory_depth = int(1e5)\n",
    "epsilon_i = 1.0\n",
    "epsilon_f = 0.1\n",
    "anneal_time = int(1e6)\n",
    "burn_in = int(5e4)\n",
    "gamma = 0.99\n",
    "learning_rate = 2.5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepQNetwork(num_frames, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(model, memory_depth, learning_rate, gamma, epsilon_i, epsilon_f, anneal_time, 'ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: model-2985892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4728272ffe5c4a82b000f8f94f94b086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return (last 100 episodes): 4.43\n",
      "Average return (last 100 episodes): 4.67\n",
      "Average return (last 100 episodes): 3.88\n",
      "Average return (last 100 episodes): 2.97\n",
      "Average return (last 100 episodes): 3.05\n",
      "Average return (last 100 episodes): 2.68\n",
      "Average return (last 100 episodes): 3.1\n",
      "Average return (last 100 episodes): 4.96\n",
      "Average return (last 100 episodes): 6.26\n",
      "Average return (last 100 episodes): 6.2\n",
      "Average return (last 100 episodes): 5.65\n",
      "Average return (last 100 episodes): 6.75\n",
      "Average return (last 100 episodes): 7.27\n",
      "Average return (last 100 episodes): 6.72\n",
      "Average return (last 100 episodes): 5.78\n",
      "Average return (last 100 episodes): 6.35\n",
      "Average return (last 100 episodes): 6.86\n",
      "Average return (last 100 episodes): 6.94\n",
      "Average return (last 100 episodes): 7.54\n",
      "Average return (last 100 episodes): 8.38\n",
      "Average return (last 100 episodes): 9.32\n",
      "Average return (last 100 episodes): 10.21\n",
      "Average return (last 100 episodes): 10.75\n",
      "Saving model before quitting...\n"
     ]
    }
   ],
   "source": [
    "metadata = agent.play(episodes, train=True, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
